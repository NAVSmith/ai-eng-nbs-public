{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e7db2b1-8f9c-46bd-9c50-b6cfb0a38a22",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "## Types of RAG eval\n",
    "\n",
    "There are at least 4 types of RAG eval that users are typically interested in (here, `<>` means \"compared against\"):\n",
    "\n",
    "1. **Response <> reference answer**: metrics like correctness measure \"*how similar/correct is the answer, relative to a ground-truth label*\"\n",
    "2. **Response <> input**: metrics like answer relevance, helpfulness, etc. measure \"*how well does the generated response address the initial user input*\"\n",
    "3. **Response <> retrieved docs**: metrics like faithfulness, hallucinations, etc. measure \"*to what extent does the generated response agree with the retrieved context*\"\n",
    "5. **Retrieved docs <> input**: metrics like score @ k, mean reciprocal rank, NDCG, etc. measure \"*how good are my retrieved results for this query*\"\n",
    "\n",
    "<div>\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_eval.png\" alt='langsmith_rag_eval' width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "## RAG pipeline \n",
    "\n",
    "To start, we build a RAG pipeline. We will be using LangChain strictly for creating the retriever and retrieving the relevant documents. The overall pipeline does not use LangChain. LangSmith works regardless of whether or not your pipeline is built with LangChain.\n",
    "\n",
    "**Note** in the below example, we return the retrieved documents as part of the final answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809e9a0-44bc-4e9f-8eee-732ef077538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %capture --no-stderr\n",
    "# ! pip install langsmith langchain-community langchain chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97fcce-136a-484d-a3da-933c1edc1583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:35:38.080837Z",
     "start_time": "2024-10-26T14:35:38.068635Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cab79-2d5e-4324-ba4a-54b6f4094cb0",
   "metadata": {},
   "source": [
    "We build an `index` using a set of LangChain docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c0017-f4dd-4071-aa48-40957ffb4e9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:36:42.854166Z",
     "start_time": "2024-10-26T14:36:29.164126Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7eb512a24d3464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:37:40.371480Z",
     "start_time": "2024-10-26T14:37:40.356483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine the content of the splits\n",
    "splits[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365fb82-78a6-40b6-bd59-daaa1e79d6c8",
   "metadata": {},
   "source": [
    "Next, we build a `RAG chain` that returns an `answer` and the retrieved documents as `contexts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e249d7-bc6c-4631-b099-6daaeeddf38a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:38:06.938420Z",
     "start_time": "2024-10-26T14:38:06.916654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### RAG\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, model: str = \"gpt-4-0125-preview\"):\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._client = wrap_openai(openai.Client())\n",
    "        self._model = model\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self.retrieve_docs(question)\n",
    "        response = self._client.chat.completions.create(\n",
    "            model=self._model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI code assistant with expertise in LCEL.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101d155-a1ab-460c-8c3e-f1f44e09a8b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:38:22.113590Z",
     "start_time": "2024-10-26T14:38:17.196185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = rag_bot.get_answer(\"What is LCEL?\")\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e8ec7-a085-4224-ad38-0087e1d553f1",
   "metadata": {},
   "source": [
    "## RAG Dataset \n",
    "\n",
    "Next, we build a dataset of QA pairs based upon the [documentation](https://python.langchain.com/docs/expression_language/) that we indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f29304f-d79b-40e9-988a-343732102af9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:39:14.390481Z",
     "start_time": "2024-10-26T14:39:13.365484Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I apply a custom function to one of the inputs of an LCEL chain?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Use RunnablePassthrough. from langchain_core.runnables import RunnableParallel, RunnablePassthrough; from langchain_core.prompts import ChatPromptTemplate; from langchain_openai import ChatOpenAI; prompt = ChatPromptTemplate.from_template('Tell a joke about: {input}'); model = ChatOpenAI(); runnable = ({'input' : RunnablePassthrough()} | prompt | model); runnable.invoke('flowers')\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"Use RunnableLambda with itemgetter to extract the relevant key. from operator import itemgetter; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableLambda; from langchain_openai import ChatOpenAI; def length_function(text): return len(text); chain = ({'prompt_input': itemgetter('foo') | RunnableLambda(length_function),} | prompt | model); chain.invoke({'foo':'hello world'})\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf3a0f-621f-468d-818d-a6f2d4b53823",
   "metadata": {},
   "source": [
    "## RAG Evaluators\n",
    "\n",
    "### Type 1: Reference Answer\n",
    "\n",
    "First, lets consider the case in which we want to compare our RAG chain answer to a reference answer.\n",
    "\n",
    "This is shown on the far right (blue):\n",
    "\n",
    "<div>\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_eval.png\" alt='langsmith_rag_eval' width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Here is the eval process we will use:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_story.png\" alt='langsmith_rag_sto' width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-answer-vs-reference\n",
    "\n",
    "![langsmith_rag_flow.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe0b4a-2a30-4f40-b3aa-5cc67c6a7802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:41:18.013066Z",
     "start_time": "2024-10-26T14:41:18.002261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52104cef-d711-4b3f-a37f-7b887213fdd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:41:20.007038Z",
     "start_time": "2024-10-26T14:41:19.470736Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt \n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get summary\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    \n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    print(f'question={input_question}, score={score}')\n",
    "    return {\"key\": \"answer_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b39ac-ff96-404e-ab92-87425f0419d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:42:11.371779Z",
     "start_time": "2024-10-26T14:41:48.079343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-3.5-turbo\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba4123-c691-4aa0-ba76-e567e8aaf09f",
   "metadata": {},
   "source": [
    "### Type 2: Answer Hallucination\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-answer-hallucination\n",
    "\n",
    "![langsmith_rag_flow_hallucination.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_flow_hallucination.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f8818-9e5e-496c-8a75-b5065113ca70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:44:11.480027Z",
     "start_time": "2024-10-26T14:44:10.955557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt \n",
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    print(f'question={input_question}, score={score}')\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2404f-af24-4f0b-9dab-95e3db8b0db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:44:39.296034Z",
     "start_time": "2024-10-26T14:44:13.740257Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a27cb-1a31-4194-b160-8cdcfbf24eea",
   "metadata": {},
   "source": [
    "### Type 3: Document Relevance to Question\n",
    "\n",
    "#### Eval flow\n",
    "\n",
    "We simply use an LLM-as-judge with an easily customized grader prompt: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-document-relevance\n",
    "\n",
    "![langsmith_rag_flow_doc_relevance.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_flow_doc_relevance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7e1ad-3e86-461f-aea7-67cbc86c3dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:47:05.010254Z",
     "start_time": "2024-10-26T14:47:04.489132Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grade prompt \n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "    \n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "        \n",
    "    # RAG answer \n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    print(f'question={input_question}, score={score}')\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6657472-eaa4-4e3a-80f7-5f6287e0e0f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:47:35.024932Z",
     "start_time": "2024-10-26T14:47:05.973023Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-doc-relevance\",\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cca574-1881-4168-8c63-2443290d89f6",
   "metadata": {},
   "source": [
    "### Type 4: Evaluating intermediate traces (retrieval)\n",
    "\n",
    "What if we didn't explicity return documents from our RAG chain?\n",
    "\n",
    "In this case, we can isolate them as intermediate chain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baeca7c-b398-481d-9eb1-ce3ea73f3d8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T14:49:10.938204Z",
     "start_time": "2024-10-26T14:48:43.216849Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def document_relevance_grader(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see if retrieved documents are relevant to the question\n",
    "    \"\"\"\n",
    "\n",
    "    # Get documents and question\n",
    "    rag_pipeline_run = next(\n",
    "        run for run in root_run.child_runs if run.name == \"get_answer\"\n",
    "    )\n",
    "    retrieve_run = next(\n",
    "        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"\n",
    "    )\n",
    "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
    "    input_question = retrieve_run.inputs[\"question\"]\n",
    "\n",
    "     # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"documents\": doc_txt})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}\n",
    "\n",
    "def answer_hallucination_grader(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG input\n",
    "    rag_pipeline_run = next(\n",
    "        run for run in root_run.child_runs if run.name == \"get_answer\"\n",
    "    )\n",
    "    retrieve_run = next(\n",
    "        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"\n",
    "    )\n",
    "    doc_txt = \"\\n\\n\".join(doc.page_content for doc in retrieve_run.outputs[\"output\"])\n",
    "\n",
    "    # RAG output\n",
    "    prediction = rag_pipeline_run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "    \n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": doc_txt,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "    \n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[document_relevance_grader, answer_hallucination_grader],\n",
    "    experiment_prefix=\"LCEL context, gpt-3.5-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c9f06b891d5ef",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You can find more Langsmith evaluation tutorials in the [official documentation](https://docs.smith.langchain.com/evaluation/tutorials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
