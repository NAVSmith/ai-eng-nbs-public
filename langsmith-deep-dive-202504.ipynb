{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dda924f-a850-416d-831d-e45a8606ce41",
   "metadata": {},
   "source": [
    "# LangSmith Evaluation Deep Dive\n",
    "\n",
    "### Summary\n",
    "\n",
    "See here for an overview of evaluation:\n",
    "https://docs.smith.langchain.com/evaluation\n",
    "\n",
    "![langsmith_summary.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_summary.png)\n",
    "\n",
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe2067-c2e4-4d50-8213-505f33c5a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U langsmith openai ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6ea90-1b8c-4a74-b992-03ef273a1a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:25:41.644600Z",
     "start_time": "2024-10-26T16:25:41.633958Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true' # enables tracing \n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb1dde-e9ee-4baf-b429-e040f5254726",
   "metadata": {},
   "source": [
    "# Dataset: Manually Curated\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How can I build my own dataset?\n",
    "\n",
    "`Setup:` \n",
    "\n",
    "Let's build a dataset of question-answer pairs on this blog post about `DBRX`:\n",
    "\n",
    "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\n",
    "\n",
    "We'll build a `Manually Curated` dataset of input, output pairs:\n",
    "\n",
    "![ai-eng/langsmith_rag_story](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/langsmith_rag_story.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09dde5e-df83-45ee-9b81-bcb09f084978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:27:24.821324Z",
     "start_time": "2024-10-26T16:27:24.817104Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"How many tokens was DBRX pre-trained on?\",\n",
    "    \"Is DBRX a MOE model and how many parameters does it have?\",\n",
    "    \"How many GPUs was DBRX trained on and what was the connectivity between GPUs?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"DBRX was pre-trained on 12 trillion tokens of text and code data.\",\n",
    "    \"Yes, DBRX is a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters.\",\n",
    "    \"DBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband\",\n",
    "]\n",
    "\n",
    "# Dataset\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Write to csv\n",
    "\n",
    "# Create directory data if it does not exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "csv_path = \"data/DBRX_eval.csv\"\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ee5c2-8b79-4684-8ca4-03abccad5ed4",
   "metadata": {},
   "source": [
    "LangSmith SDK docs:\n",
    "\n",
    "* https://docs.smith.langchain.com/evaluation/quickstart#1-create-a-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b2529-0064-43f8-a89b-8a7a4606f53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:31:40.069355Z",
     "start_time": "2024-10-26T16:31:38.759938Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about DBRX model.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a9bde-126e-45b1-be48-7109a9394f58",
   "metadata": {},
   "source": [
    "Update dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c1da0-06c1-4a71-99d4-80ed5a760cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:00.310317Z",
     "start_time": "2024-10-26T16:32:00.049838Z"
    }
   },
   "outputs": [],
   "source": [
    "new_questions = [\n",
    "    \"What is the context window of DBRX Instruct?\",\n",
    "]\n",
    "\n",
    "new_answers = [\n",
    "    \"DBRX Instruct was trained with up to a 32K token context window.\",\n",
    "]\n",
    "\n",
    "# See updated version in the UI\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in new_questions],\n",
    "    outputs=[{\"answer\": a} for a in new_answers],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d5970-45f9-45f1-ba9d-25b349eee12b",
   "metadata": {},
   "source": [
    "We can also create a dataset directly from a csv with the LangSmith UI.\n",
    "\n",
    "LangSmith UI docs:\n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/faq/manage-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f74eaf-0aa3-4e0a-9124-ad1f13b5e813",
   "metadata": {},
   "source": [
    "# Dataset: From User Logs\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How can I save user logs as a dataset for future testing?\n",
    "\n",
    "![ai-eng/userlogs.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/userlogs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12917250-7f36-4a3f-b7d2-9debba9a27ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:07.114748Z",
     "start_time": "2024-10-26T16:32:07.112971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new project where user question are logged\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"DBRX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bbdc9-62b5-4f78-ba41-f7c0f861b4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:10.121563Z",
     "start_time": "2024-10-26T16:32:09.606365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load blog post\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14156d0-e377-4112-bf67-d0efe91b1258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:15.132673Z",
     "start_time": "2024-10-26T16:32:14.786346Z"
    }
   },
   "outputs": [],
   "source": [
    "# OpenAI API\n",
    "\n",
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "def answer_dbrx_question_oai(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on a provided website text using OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (dict): A dictionary with a single key 'question', representing the user's question as a string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a single key 'output', containing the generated answer as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt\n",
    "    system_msg = (\n",
    "        f\"Answer user questions in 2-3 sentences about this context: \\n\\n\\n {full_text}\"\n",
    "    )\n",
    "\n",
    "    # Pass in website text\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": inputs[\"question\"]},\n",
    "    ]\n",
    "\n",
    "    # Call OpenAI\n",
    "    response = openai_client.chat.completions.create(\n",
    "        messages=messages, model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response.dict()[\"choices\"][0][\"message\"][\"content\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcb0a7-6b2f-4a4c-8941-18f3297171a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:20.374903Z",
     "start_time": "2024-10-26T16:32:17.927933Z"
    }
   },
   "outputs": [],
   "source": [
    "# User question example\n",
    "\n",
    "answer_dbrx_question_oai(\n",
    "    {\n",
    "        \"question\": \"What are the main differences in training efficiency between MPT-7B vs DBRX?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec9ef03-633e-4c63-ba64-c359318da46b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:23.635076Z",
     "start_time": "2024-10-26T16:32:22.700319Z"
    }
   },
   "outputs": [],
   "source": [
    "# User question example\n",
    "\n",
    "answer_dbrx_question_oai({\"question\": \"How many tokens was DBRX pre-trained on?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ba15c-e1fe-4ae7-b1cd-fd82a2ae31b5",
   "metadata": {},
   "source": [
    "# LLM-as-Judge: Built-in evaluator\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How can I evaluate the my LLM against my dataset?\n",
    "\n",
    "`Evaluation flow`\n",
    "\n",
    "![ai-eng/llm-as-judge.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/llm-as-judge.png)\n",
    "\n",
    "`Built-in evaluator`\n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/faq/evaluator-implementations\n",
    "\n",
    "`CoT_qa`\n",
    "\n",
    "```\n",
    "Use chain of thought \"reasoning\" before determining a final verdict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a620b1-c2ad-42ff-b6c2-f1d25b112d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:32:56.248819Z",
     "start_time": "2024-10-26T16:32:49.193293Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-oai\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c226f2-9591-47f4-b8bc-a98539f414bd",
   "metadata": {},
   "source": [
    "`What did we do?`\n",
    "\n",
    "![ai-eng/llm-as-judge2.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/llm-as-judge2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de220436-82dd-4a25-acf9-ea354b14ed30",
   "metadata": {},
   "source": [
    "# Custom evaluator\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How can I define my own custom evaluator? \n",
    "\n",
    "Let's say we want to define a simple assertion that an answer is actually generated.\n",
    "\n",
    "![ai-eng/custom-evaluator.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/custom-evaluator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be0869-d1d3-419f-b404-c051063cd22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:34:07.650294Z",
     "start_time": "2024-10-26T16:34:05.208327Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "\n",
    "    # Check if the student_answer is an empty string\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [is_answered]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-is-answered\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f416863-4b0b-4601-8551-c6ea667b0ade",
   "metadata": {},
   "source": [
    "# Comparison \n",
    "\n",
    "`Question:` \n",
    "\n",
    "How does `Mistral-7b` running locally compare to `GPT-3.5-turbo` for question-answering?\n",
    " \n",
    "`Setup:`\n",
    "\n",
    "https://github.com/ollama/ollama-python\n",
    "\n",
    "After installing it, you will need to run the server:\n",
    "\n",
    "`ollama serve`\n",
    "\n",
    "if you installed it on a Mac:\n",
    "\n",
    "`/opt/homebrew/opt/ollama/bin/ollama serve`\n",
    "\n",
    "then pull the mistral model: `ollama pull mistral`\n",
    "\n",
    "Instrument Ollama calls with LangSmith: \n",
    "\n",
    "https://docs.smith.langchain.com/cookbook/tracing-examples/traceable#using-the-decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4064591-5af8-4b51-ac92-e4ea7ad0992c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:52:08.145700Z",
     "start_time": "2024-10-26T16:51:39.201572Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mistral\n",
    "\n",
    "import ollama\n",
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_ollama(messages, model: str):\n",
    "    stream = ollama.chat(messages=messages, model=\"mistral\", stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "        response = response + chunk[\"message\"][\"content\"]\n",
    "    return response\n",
    "\n",
    "\n",
    "def answer_dbrx_question_mistral(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on a provided website text using Ollama serving Mistral locally.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (dict): A dictionary with a single key 'question', representing the user's question as a string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a single key 'output', containing the generated answer as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt\n",
    "    system_msg = f\"Answer user questions about this context: \\n\\n\\n {full_text}\"\n",
    "\n",
    "    # Pass in website text\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f'Answer the question in 2-3 sentences {inputs[\"question\"]}',\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Call Mistral\n",
    "    response = call_ollama(messages, model=\"mistral\")\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "result = answer_dbrx_question_mistral(\n",
    "    {\n",
    "        \"question\": \"What are the main differences in training efficiency between MPT-7B vs DBRX?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8616d7e-7b51-44a7-aad0-86c8225b7ab5",
   "metadata": {},
   "source": [
    "What are we doing?\n",
    "\n",
    "![ai-eng/comparison.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd975c8-6052-4320-81a9-659ee9cd692d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:52:54.705627Z",
     "start_time": "2024-10-26T16:52:16.482508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluators\n",
    "qa_evalulator = [LangChainStringEvaluator(\"cot_qa\")]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_mistral,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into mistral\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec06132-eeca-42d4-a1da-50b952455c3f",
   "metadata": {},
   "source": [
    "Use comparison view to inspect results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f7a09-0eba-4c4c-b60e-64777d3438ad",
   "metadata": {},
   "source": [
    "# Experiment on datasets from the prompt playground (no code)\n",
    "\n",
    "We've showed various ways to run evals using the SDK.\n",
    " \n",
    "But sometimes I want to do more rapid testing.\n",
    "\n",
    "For this I can use the LangSmith prompt hub directly: \n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/faq/experiments-app\n",
    "\n",
    "Here is a problem I've worked on recently: \n",
    "\n",
    "I want to grade documents in a RAG chain that takes as input: (1) A document and (2) A question.\n",
    " \n",
    "And returns: (3) JSON with `score` yes or no that tells me if the documents are related to a question. \n",
    "\n",
    "See notebooks [here](https://github.com/langchain-ai/langgraph/tree/main/examples/rag).\n",
    "\n",
    "![ai-eng/experiment.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/experiment.png)\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How do different LLMs perform at instruction following to produce a JSON output?\n",
    "\n",
    "First, I build a dataset of test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb9fbf-06e0-4eaf-94fb-5d6a8dc3d2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:53:41.169218Z",
     "start_time": "2024-10-26T16:53:41.164599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# relevance check\n",
    "inputs = [\n",
    "    {\n",
    "        \"question\": \"agent memory\",\n",
    "        \"doc_txt\": \"agent memory has two types: short and long term\",\n",
    "    },\n",
    "    {\"question\": \"hallucinations\", \"doc_txt\": \"DBRX was pretrained on 12T tokens\"},\n",
    "    {\n",
    "        \"question\": \"DBRX content window\",\n",
    "        \"doc_txt\": \"DBRX has a 32K token context window\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = [\"yes\", \"no\", \"yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d2fe9-7784-4b01-8bfd-f966545364ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:53:49.093216Z",
     "start_time": "2024-10-26T16:53:47.972706Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Relevance_grade\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Testing relevance grading.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c4180-79c5-46fe-be43-b5693fa09ec5",
   "metadata": {},
   "source": [
    "Test prompt in the [Prompt Hub](https://smith.langchain.com/hub/rlm/score_documents?organizationId=1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8).\n",
    "\n",
    "```\n",
    "SYSTEM\n",
    "\n",
    "You are a grader assessing relevance of a retrieved document to a user question. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "\n",
    "HUMAN\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Document: {doc_txt}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe16ef-a1e1-4111-8763-88cfef36ba4f",
   "metadata": {},
   "source": [
    "# Attach evaluators to datasets (no code)\n",
    "\n",
    "From part 8, we:\n",
    "\n",
    "(1) Set up a dataset of test cases for document grading \n",
    "\n",
    "(2) Ran experiments from the prompt hub\n",
    "\n",
    "(3) Manually reviewed them\n",
    "\n",
    "But, we can go one step further:\n",
    "\n",
    "We can attach an LLM evaluator to our dataset. \n",
    "\n",
    "This is automatically applied for every experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc476a-7b7b-472c-9363-dff7613f53f4",
   "metadata": {},
   "source": [
    "Grade prompt:\n",
    "\n",
    "```\n",
    "You are a grader. You will be shown: \n",
    "\n",
    "(1) Submission: a student submission for a JSON string\n",
    "\n",
    "(2) Reference: the ground truth value expected in the JSON string\n",
    "\n",
    "The student is producing a JSON with a single key \"score\" to indicate whether doc_text is relevant to question for this input:\n",
    "\n",
    "[Input]: {input}\n",
    "\n",
    "Grade the student as correct if that the student submission is valid JSON (or a JSON string) and contains the Reference value. If the student submission contains a preamble of text \"e.g., 'sure, here is the JSON'\" then score that as incorrect because we only want to JSON returned.\n",
    "\n",
    "[BEGIN DATA]\n",
    "\n",
    "***\n",
    "\n",
    "[Submission]: {output}\n",
    "\n",
    "***\n",
    "\n",
    "[Reference]: {reference}\n",
    "\n",
    "***\n",
    "\n",
    "[END DATA]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe5dc6-9aaf-495f-85b3-bde7fad92de7",
   "metadata": {},
   "source": [
    "# Summary Evaluators\n",
    "\n",
    "We previously talked about using retrieval grading as part of RAG:\n",
    "\n",
    "![ai-eng/summary.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/summary.png)\n",
    "\n",
    "In short, we use an LLM to grader whether a document is relevant to input question.\n",
    "\n",
    "This returns a binary `yes` or `no`.\n",
    "\n",
    "We built an eval set and ground truth is a binary `yes` or `no` for each example:\n",
    "\n",
    "https://smith.langchain.com/public/ad300ffb-8bf5-450a-9c26-1b34481fb709/d\n",
    "\n",
    "`Question:`\n",
    "\n",
    "How can I create a custom metric to summarize performance on this dataset?\n",
    "\n",
    "![ai-eng/summary2.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/summary2.png)\n",
    "\n",
    "First, let's set up the two chains we want to compare:\n",
    "\n",
    "* [OpenAI w/ tool use](https://github.com/langchain-ai/langgraph/blob/e779b4335b8a8b11c9e8ac71b89e9a08a94e3ff9/examples/rag/langgraph_self_rag.ipynb)\n",
    "* [Mistral w/ JSON mode running locally](https://github.com/langchain-ai/langgraph/blob/e779b4335b8a8b11c9e8ac71b89e9a08a94e3ff9/examples/rag/langgraph_self_rag_local.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2cc78-4c74-4137-8430-87e571a13985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:55:33.782107Z",
     "start_time": "2024-10-26T16:55:33.735418Z"
    }
   },
   "outputs": [],
   "source": [
    "### OpenAI Grader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_oai = grade_prompt | structured_llm_grader\n",
    "\n",
    "\n",
    "def predict_oai(inputs: dict) -> dict:\n",
    "    # Returns pydantic object\n",
    "    grade = retrieval_grader_oai.invoke(\n",
    "        {\"question\": inputs[\"question\"], \"document\": inputs[\"doc_txt\"]}\n",
    "    )\n",
    "    return {\"grade\": grade.score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c200e19-8f6b-4018-88c3-03a7dc180dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:55:39.160320Z",
     "start_time": "2024-10-26T16:55:39.037787Z"
    }
   },
   "outputs": [],
   "source": [
    "### Mistral Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"mistral\", format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader_mistral = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "def predict_mistral(inputs: dict) -> dict:\n",
    "    # Returns JSON\n",
    "    grade = retrieval_grader_mistral.invoke(\n",
    "        {\"question\": inputs[\"question\"], \"document\": inputs[\"doc_txt\"]}\n",
    "    )\n",
    "    return {\"grade\": grade[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe3313-19bb-423a-8538-c064dd64b176",
   "metadata": {},
   "source": [
    "Documentation: \n",
    "\n",
    "https://docs.smith.langchain.com/evaluation/faq/custom-evaluators#summary-evaluators\n",
    "\n",
    "We can define a custom summary metric over the dataset.\n",
    "\n",
    "[`Precision` and `Recall` are common metrics to evaluate a binary clasification](https://en.wikipedia.org/wiki/Precision_and_recall):\n",
    "\n",
    "* `Precision`: True positives (`TP`) / All positives (`TP + False Positives (FP)`).\n",
    "* `Recall`: `TP` / All samples that should have been identified as positive\n",
    "\n",
    "`F1` considers both the precision and the recall of the test to compute the score:\n",
    "* `F1` score is the harmonic mean of precision and recall, and it reaches its best value at 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c5ed8-328a-4d54-8c36-a9b7e12be2ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:55:43.172319Z",
     "start_time": "2024-10-26T16:55:43.167636Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langsmith.schemas import Example, Run\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "def f1_score_summary_evaluator(runs: List[Run], examples: List[Example]) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates the F1 score for a list of runs against a set of examples.\n",
    "\n",
    "    The function iterates through paired runs and examples, comparing the output\n",
    "    of each run (`run.outputs[\"grade\"]`) with the expected output in the example\n",
    "    (`example.outputs[\"answer\"]`). It calculates the true positives, false positives,\n",
    "    and false negatives based on these comparisons to compute the F1 score of the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - runs (List[Run]): A list of run objects, where each run contains an output that is a prediction.\n",
    "    - examples (List[Example]): A list of example objects, where each example contains an output that is the expected answer.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with a single key-value pair where the key is \"f1_score\" and the value\n",
    "    \"\"\"\n",
    "\n",
    "    # Default values\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    # Iterate through samples\n",
    "    for run, example in zip(runs, examples):\n",
    "        reference = example.outputs[\"answer\"]\n",
    "        prediction = run.outputs[\"grade\"]\n",
    "        if reference and prediction == reference:\n",
    "            true_positives += 1\n",
    "        elif prediction and not reference:\n",
    "            false_positives += 1\n",
    "        elif not prediction and reference:\n",
    "            false_negatives += 1\n",
    "    if true_positives == 0:\n",
    "        return {\"key\": \"f1_score\", \"score\": 0.0}\n",
    "\n",
    "    # Compute F1 score\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"key\": \"f1_score\", \"score\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81ab45-2f09-447b-b1b4-91deea14f57a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:56:01.137623Z",
     "start_time": "2024-10-26T16:55:48.838778Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    predict_mistral,\n",
    "    data=\"Relevance_grade\",\n",
    "    summary_evaluators=[f1_score_summary_evaluator],\n",
    "    experiment_prefix=\"test-score-mistral\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"model\": \"mistral\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ed013-438f-46d9-8067-0a30f33d006e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:56:03.650347Z",
     "start_time": "2024-10-26T16:56:01.152789Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    predict_oai,\n",
    "    data=\"Relevance_grade\",\n",
    "    summary_evaluators=[f1_score_summary_evaluator],\n",
    "    experiment_prefix=\"test-score-oai\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"model\": \"oai\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7ffe7c6432a9e",
   "metadata": {},
   "source": [
    "On the Langsmith website, select both experiments by clicking on the small box on the left, and then select \"Compare\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590debc-93a5-4d1f-9bde-53e91834aea9",
   "metadata": {},
   "source": [
    "# Evaluating RAG\n",
    "\n",
    "See our [RAG guide](https://docs.smith.langchain.com/cookbook/testing-examples/rag_eval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8e6ba-e8eb-46fc-973f-f20143d7056b",
   "metadata": {},
   "source": [
    "# Regression testing\n",
    "\n",
    "Previously, we talked about various types of RAG evaluations.\n",
    "\n",
    "`Question:` \n",
    "\n",
    "How can I assess whether a new LLM (e.g., phi3), can I be used in my RAG chain?\n",
    "\n",
    "For this, regression testing is highly useful.\n",
    "\n",
    "It lets us easily pinpoint changes in performance in our eval set across model versions.\n",
    "\n",
    "First, define an eval set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534eb07-975a-4760-a99b-3c773530e963",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:57:47.029926Z",
     "start_time": "2024-10-26T16:57:47.028067Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG_repetitions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b7790-83fa-4d34-a494-369a9f65b45e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:57:51.618825Z",
     "start_time": "2024-10-26T16:57:50.663251Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"My LCEL map contains the key 'question'. What is the difference between using itemgetter('question'), lambda x: x['question'], and x.get('question')?\",\n",
    "    \"How can I make the output of my LCEL chain a string?\",\n",
    "    \"How can I run two LCEL chains in parallel and write their output to a map?\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"Itemgetter can be used as shorthand to extract specific keys from the map. In the context of a map operation, the lambda function is applied to each element in the input map and the function returns the value associated with the key 'question'. (get) is safer for accessing values in a dictionary because it handles the case where the key might not exist.\",\n",
    "    \"Use StrOutputParser. from langchain_openai import ChatOpenAI; from langchain_core.prompts import ChatPromptTemplate; from langchain_core.output_parsers import StrOutputParser; prompt = ChatPromptTemplate.from_template('Tell me a short joke about {topic}'); model = ChatOpenAI(model='gpt-3.5-turbo') #gpt-4 or other LLMs can be used here; output_parser = StrOutputParser(); chain = prompt | model | output_parser\",\n",
    "    \"We can use RunnableParallel. For example: from langchain_core.prompts import ChatPromptTemplate; from langchain_core.runnables import RunnableParallel; from langchain_openai import ChatOpenAI; model = ChatOpenAI(); joke_chain = ChatPromptTemplate.from_template('tell me a joke about {topic}') | model; poem_chain = (ChatPromptTemplate.from_template('write a 2-line poem about {topic}') | model); map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain); map_chain.invoke({'topic': 'bear'})\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"RAG_QA_LCEL\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about LCEL.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8f803-6aef-41b3-861d-3c049d214679",
   "metadata": {},
   "source": [
    "RAG chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364de79-95c3-422c-b4b3-ba66c073b690",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:58:13.239577Z",
     "start_time": "2024-10-26T16:57:59.653955Z"
    }
   },
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n",
    "full_doc_text = ' ---- '.join([d.page_content for d in docs])\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e0cfa-8380-4a59-b40d-f0525956c3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:58:13.247483Z",
     "start_time": "2024-10-26T16:58:13.241236Z"
    }
   },
   "outputs": [],
   "source": [
    "### RAG\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "class RagBot:\n",
    "    \"\"\"\n",
    "    A class to interface with retrieval-augmented generation (RAG) models from different providers\n",
    "    such as OpenAI or Ollama, utilizing a retriever for document-based context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever,\n",
    "        provider: str = \"openai\",\n",
    "        model: str = \"gpt-4-0125-preview\",\n",
    "        use_vectorstore: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RagBot with a retriever, provider information, model details, and configuration\n",
    "        to use a vector store for document retrieval.\n",
    "\n",
    "        Args:\n",
    "        retriever: The document retriever instance.\n",
    "        provider (str): The provider of the RAG model ('openai' or 'ollama').\n",
    "        model (str): The model identifier used by the provider.\n",
    "        use_vectorstore (bool): Flag to determine whether to use vectorstore for document retrieval.\n",
    "        \"\"\"\n",
    "        self._retriever = retriever\n",
    "        self._provider = provider\n",
    "        self._model = model\n",
    "        self._use_vectorstore = use_vectorstore\n",
    "        if provider == \"openai\":\n",
    "            self._client = wrap_openai(openai.Client())\n",
    "        elif provider == \"ollama\":\n",
    "            self._client = ChatOllama(model=model, temperature=0)\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        \"\"\"\n",
    "        Retrieves documents based on the input question, using either a vectorstore or full context.\n",
    "\n",
    "        Args:\n",
    "        question (str): The question to retrieve documents for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of documents relevant to the question or the full context (as a string).\n",
    "        \"\"\"\n",
    "        if self._use_vectorstore:\n",
    "            return self._retriever.invoke(question)\n",
    "        else:\n",
    "            return full_doc_text\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        \"\"\"\n",
    "        Generates an answer for a given question by using RAG, leveraging both the retriever\n",
    "        and the provider's model capabilities.\n",
    "\n",
    "        Args:\n",
    "        question (str): The user's question to answer.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing the 'answer' and 'contexts' (related documents).\n",
    "        \"\"\"\n",
    "        similar = self.retrieve_docs(question)\n",
    "        if self._provider == \"openai\":\n",
    "            \"OpenAI RAG\"\n",
    "            response = self._client.chat.completions.create(\n",
    "                model=self._model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful AI code assistant with expertise in LCEL.\\n\"\n",
    "                        \" Use the following docs to produce a concise code solution to the user question.\\n\"\n",
    "                        \" Use three sentences maximum and keep the answer concise. \\n\"\n",
    "                        f\"## Docs\\n\\n{similar}\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": question},\n",
    "                ],\n",
    "            )\n",
    "            response_str = response.choices[0].message.content\n",
    "\n",
    "        elif self._provider == \"ollama\":\n",
    "            \"Ollama RAG\"\n",
    "            prompt = PromptTemplate(\n",
    "                template=\"\"\"You are a helpful AI code assistant with expertise in LCEL.\n",
    "                Use the following docs to produce a concise code solution to the user question.\n",
    "                If you don't know the answer, just say that you don't know. \n",
    "                Use three sentences maximum and keep the answer concise.\n",
    "                Question: {question} \n",
    "                Context: {context} \n",
    "                Answer: \"\"\",\n",
    "                input_variables=[\"question\", \"context\"],\n",
    "            )\n",
    "            rag_chain = prompt | self._client | StrOutputParser()\n",
    "            response_str = rag_chain.invoke({\"context\": similar, \"question\": question})\n",
    "\n",
    "        return {\n",
    "            \"answer\": response_str,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b839a-fb48-4700-9e26-a8fb6fc2d9e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:59:43.312273Z",
     "start_time": "2024-10-26T16:59:43.308499Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rag_answer_oai(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot(retriever, provider=\"openai\", model=\"gpt-4-0125-preview\")\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "def predict_rag_answer_llama3(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot(retriever, provider=\"ollama\", model=\"llama3\")\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "\n",
    "def predict_rag_answer_phi3(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    rag_bot = RagBot(retriever, provider=\"ollama\", model=\"phi3\")\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c34588-216c-48c2-8e3d-ac3ad860191a",
   "metadata": {},
   "source": [
    "Define evaluator:\n",
    "\n",
    "Note, the evaluator is checked in here: \n",
    "\n",
    "https://smith.langchain.com/hub/langchain-ai/rag-answer-accuracy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b0c50-afd4-4d2e-8dc6-0804b07e668f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:59:46.429762Z",
     "start_time": "2024-10-26T16:59:46.425091Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "def answer_evaluator(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer generation\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, answer, and reference answer\n",
    "    rag_pipeline_run = next(\n",
    "        run for run in root_run.child_runs if run.name == \"get_answer\"\n",
    "    )\n",
    "    retrieve_run = next(\n",
    "        run for run in rag_pipeline_run.child_runs if run.name == \"retrieve_docs\"\n",
    "    )\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"answer\"]\n",
    "    prediction = rag_pipeline_run.outputs[\"answer\"]\n",
    "\n",
    "    # Data model for grade\n",
    "    class GradeAnswer(BaseModel):\n",
    "        \"\"\"A numerical score for answer accuracy.\"\"\"\n",
    "\n",
    "        score: int = Field(\n",
    "            description=\"Answer matches the grond truth, score from 1 to 10\"\n",
    "        )\n",
    "\n",
    "    # LLM with function call, use highest capacity model\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0)  #\n",
    "    structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "    # Prompt\n",
    "    system = \"\"\"Is the Assistant's Answer grounded in and similar to the Ground Truth answer. Note that we do not expect all of the text \n",
    "            in code solution examples to be identical. We expect (1) code imports to be identical if the same import is used. (2) But, it is\n",
    "            ok if there are differences in the implementation itself. The main point is that the same concept is employed. A score of 1 means \n",
    "            that the Assistant answer is not at all conceptically grounded in and similar to the Ground Truth answer. A score of 5 means  that the Assistant \n",
    "            answer contains some information that is conceptically grounded in and similar to the Ground Truth answer. A score of 10 means that the \n",
    "            Assistant answer is fully conceptically grounded in and similar to the Ground Truth answer.\"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Ground Truth answer: \\n\\n {reference} \\n\\n Assistant's Answer: {prediction}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer_grader = grade_prompt | structured_llm_grader\n",
    "    score = answer_grader.invoke({\"reference\": reference, \"prediction\": prediction})\n",
    "    return {\"key\": \"answer_accuracy\", \"score\": int(score.score) / 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc17fe-7824-47c4-90a6-1317759f7f9a",
   "metadata": {},
   "source": [
    "#### Compare OpenAI vs Ollama (open source LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a58ca-5f67-4f08-bac0-10312155646f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:58:36.383230Z",
     "start_time": "2024-10-26T16:58:25.593762Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "dataset_name = \"RAG_QA_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_oai,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-gpt4-0125\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceea0c4-a466-4e30-b862-97e6b2949695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:59:52.625403Z",
     "start_time": "2024-10-26T16:59:51.238779Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_llama3,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-llama3\",\n",
    "    metadata={\"variant\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05dc73f-d86f-4718-b950-4bf6e3e540b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T16:58:39.247319Z",
     "start_time": "2024-10-26T16:58:37.901235Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_phi3,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-qa-phi3\",\n",
    "    metadata={\"variant\": \"LCEL context, phi3\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc1ab2-bde5-43d5-8394-c620c370622d",
   "metadata": {},
   "source": [
    "# Online Evaluators\n",
    "\n",
    "Sometimes we want to evaluate generations as they are logged to a project.\n",
    "\n",
    "There are a few common applications for online evaluation:\n",
    "\n",
    "* [RAG: answer hallucinations](https://smith.langchain.com/hub/rlm/rag-answer-hallucination?organizationId=1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8)\n",
    "* [RAG: document relevance](https://smith.langchain.com/hub/rlm/rag-document-relevance/playground?organizationId=1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8&type=structured)\n",
    "\n",
    "![ai-eng/onlineevaluators.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/onlineevaluators.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66206fb1-e98f-46e4-8854-95eb063ba4d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T17:06:14.066947Z",
     "start_time": "2024-10-26T17:06:11.130651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test our RAG bot\n",
    "rag_bot = RagBot(retriever, provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "response = rag_bot.get_answer(\"How to define an RAG chain in LCEL?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f530ddc-2c1c-4b0c-a755-5de08b3c3030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T17:06:15.733135Z",
     "start_time": "2024-10-26T17:06:15.730770Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG_online_eval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1e8c8-813d-40a0-93e9-a82ccf650f1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T17:06:17.984072Z",
     "start_time": "2024-10-26T17:06:17.142366Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith import traceable, Client\n",
    "import uuid\n",
    "\n",
    "client = openai.Client()\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"chain\",\n",
    "    name=\"rag\",\n",
    ")\n",
    "def rag(question: str, documents):\n",
    "    return (\n",
    "        client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Answer questions based on these documents: {documents}\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "\n",
    "rag(\"where did harrison work\", [\"ankush and his friend worked at kensho\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97211fd3-4611-434a-8788-46810218e0ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T17:06:25.726596Z",
     "start_time": "2024-10-26T17:06:24.982998Z"
    }
   },
   "outputs": [],
   "source": [
    "rag(\"where did ankush work\", [\"ankush and his friend worked at kensho\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f8b74-91df-4070-9148-23c88a4ede86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag(\"where did lance work\", [\"ankush and his friend worked at kensho\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
