{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-XDGL6Oi6h4"
      },
      "source": [
        "# LangChain Multi-Query for RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwwN9O48Aefr"
      },
      "source": [
        "## Introduction\n",
        "One of the advanced features in LangChain is Multi-Query for RAG, which allows for more efficient and effective information retrieval from large datasets.\n",
        "\n",
        "## What is Multi-Query for RAG?\n",
        "\n",
        "Multi-Query for RAG is a technique used to enhance the retrieval process in retrieval-augmented generation systems. It involves generating multiple queries from a single input and using these queries to retrieve a more diverse and relevant set of documents from the database. This approach improves the quality of the retrieved information, leading to better and more accurate generated responses.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Query Generation**: Instead of generating a single query, the system creates multiple queries based on different aspects or perspectives of the input. These queries can be variations or expansions of the original input.\n",
        "\n",
        "2. **Document Retrieval**: Each generated query is used to search the indexed documents. The results from all queries are combined to form a more comprehensive set of retrieved documents.\n",
        "\n",
        "3. **Response Generation**: The retrieved documents are then used to generate the final response. By incorporating information from multiple queries, the generated response is more likely to be accurate and relevant.\n",
        "\n",
        "## Benefits of Multi-Query for RAG\n",
        "\n",
        "- **Improved Diversity**: Multiple queries increase the chances of retrieving diverse pieces of information, covering different aspects of the input topic.\n",
        "- **Enhanced Relevance**: Combining results from multiple queries can lead to a more relevant and contextually appropriate set of documents.\n",
        "- **Robustness**: Multi-query retrieval is more robust to variations and ambiguities in the input, as different queries can capture different interpretations.\n",
        "\n",
        "## Example Workflow\n",
        "\n",
        "1. **Input Processing**: The user provides an input query.\n",
        "2. **Query Generation**: The system generates multiple queries from the input.\n",
        "3. **Document Retrieval**: Each query is used to search the document index, and the results are combined.\n",
        "4. **Response Generation**: The retrieved documents are used by the language model to generate the final response.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/images-langchain-4/muti.avif\" alt='auto' width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "## Example Code\n",
        "\n",
        "Here is a simplified example of how Multi-Query for RAG might be implemented using LangChain:\n",
        "\n",
        "```python\n",
        "from langchain.chains import MultiQueryRAGChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.indexes import VectorIndex\n",
        "\n",
        "# Define the base prompt template\n",
        "base_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"input_text\"],\n",
        "    template=\"Generate multiple queries from the following input: {input_text}\"\n",
        ")\n",
        "\n",
        "# Initialize the language model\n",
        "llm = OpenAI(model=\"text-davinci-003\")\n",
        "\n",
        "# Create the Multi-Query RAG Chain\n",
        "multi_query_chain = MultiQueryRAGChain(\n",
        "    prompt=base_prompt_template,\n",
        "    llm=llm,\n",
        "    index=VectorIndex()  # Assuming VectorIndex is already populated with documents\n",
        ")\n",
        "\n",
        "# Input text\n",
        "input_text = \"Tell me about the history of artificial intelligence.\"\n",
        "\n",
        "# Run the chain and get the response\n",
        "response = multi_query_chain.run({\"input_text\": input_text})\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPmfrdJ9_2YA"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Py-rVqx-I0"
      },
      "source": [
        "We will download an existing dataset from Hugging Face Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCfqD44TAmcx"
      },
      "outputs": [],
      "source": [
        "!pip install datasets langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "e08070cc35714354ad5c0a69e53d9947",
            "9cdb2db999324dfaa2b2cf20dd2a06f8",
            "10b7eab2663a461c901009fbcda4dcb8",
            "aaf388da79f2499096df30b3105f4fa0",
            "091d0834cb534179a87ee78bff11b901",
            "5388999d70cc4a519af6e2eda17a178a",
            "ea0e114752ff4908810601364b67da14",
            "29b09e4ff2ab4c319d110ba3054aef11",
            "3919c58ce0f7471cb57d8348aec1c6e1",
            "60059c757c3349b2a3a3ca97e83b94b2",
            "800d5d65c589426ba0e6d1b358606e94",
            "246debe1f38043469553b93dfb91c314",
            "d0ea281524ad4b97ac3bec2e0786173b",
            "fff59d139c2b4ecd8fd0645c0a0002cb",
            "b4bf703c0ffa4249968675aa5470a759",
            "b59f8a43763446af8ba8758027e4a0c6",
            "5f41a16064db4fe29c78aec7c1f09802",
            "072f8aaef6494d27aa79c5bcbd567561",
            "7ed2641c69d048eda5101673ff55f887",
            "1170a8ca1ed34ba5b40cecd0fbf2a7ed",
            "3a2f7fcc97fc45c294650cb2b662ddaa",
            "a525b7184ea7468dafb6606085888974"
          ]
        },
        "id": "iatOGmKgz8NE",
        "outputId": "c5bcab77-91b2-4e4d-b81b-bf94b5f7eae7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P7E6JYtb0cW7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "docs = []\n",
        "\n",
        "for row in data:\n",
        "    doc = Document(\n",
        "        page_content=row[\"chunk\"],\n",
        "        metadata={\n",
        "            \"title\": row[\"title\"],\n",
        "            \"source\": row[\"source\"],\n",
        "            \"id\": row[\"id\"],\n",
        "            \"chunk-id\": row[\"chunk-id\"],\n",
        "            \"text\": row[\"chunk\"]\n",
        "        }\n",
        "    )\n",
        "    docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iugHor3KAefu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
        "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb540kEs_6PZ"
      },
      "source": [
        "## Embedding and Vector DB Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlKEmBZMBxtd"
      },
      "source": [
        "Initialize our embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ6vTiDPBznz",
        "outputId": "bf5a3378-1d4f-493b-8710-aae7fa6c0a80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-d00f2f1001bc>:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embed = OpenAIEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "model_name = \"text-embedding-ada-002\"\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IurEkeeI-IYl"
      },
      "source": [
        "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEguDbUnBl6T",
        "outputId": "e9d56191-5a85-41d2-f25d-49ecdf82df1d"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-pinecone pinecone-notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e93580_4Aefw"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "# configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW52Z3YxAefw"
      },
      "source": [
        "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fG1mZ5u3Aefw"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlX9hBkDAefx"
      },
      "source": [
        "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL3KFF9E9Qb_",
        "outputId": "e5851a4f-431f-4b31-e3e5-4e535878e1c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "index_name = \"langchain-multi-query-demo\"\n",
        "existing_indexes = [\n",
        "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
        "]\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in existing_indexes:\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of ada 002\n",
        "        metric='dotproduct',\n",
        "        spec=spec\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "time.sleep(1)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3B7dHsd6QcP"
      },
      "source": [
        "Populate our index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7Yi2YGBpTWf",
        "outputId": "4567426a-0f38-4f90-f159-833a04f7d635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41584"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "thfCYHuSpW4H"
      },
      "outputs": [],
      "source": [
        "# if you want to speed things up to follow along\n",
        "#docs = docs[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sJUgrjdCVZh"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b9da1d88b0b14b5d864e4796dd140f96",
            "aa52a5b5c7f049beb8cfc2e0b8501195",
            "6561977a1cb64ca88def12daf404e734",
            "7aadc4a1a3a94a36a8ffa617816c8cde",
            "46698efe179049ef89466157c2652862",
            "093761c9141343d39791e13075aeb356",
            "ad5fcb675fbf4439a5e34b478ca960f4",
            "36104de65f204fa3b287e2ff2d43aab2",
            "3239ead1970045659a4dee532c16a46a",
            "769ff96fd8ee4a5b83ecfbc31176ad83",
            "22ef7e367f074b1cb4f231322a83728d"
          ]
        },
        "id": "HXVVU97C6SwT",
        "outputId": "48e349d9-ed26-443d-8754-0e860ce494fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9da1d88b0b14b5d864e4796dd140f96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/416 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for i in tqdm(range(0, len(docs), batch_size)):\n",
        "    i_end = min(len(docs), i+batch_size)\n",
        "    docs_batch = docs[i:i_end]\n",
        "    # get IDs\n",
        "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
        "    # get text and embed\n",
        "    texts = [d.page_content for d in docs_batch]\n",
        "    embeds = embed.embed_documents(texts=texts)\n",
        "    # get metadata\n",
        "    metadata = [d.metadata for d in docs_batch]\n",
        "    to_upsert = zip(ids, embeds, metadata)\n",
        "    index.upsert(vectors=to_upsert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FbngTBzAAU-"
      },
      "source": [
        "## Multi-Query with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVVYr13n_Ot2"
      },
      "source": [
        "Now we switch across to using our populated index as a vectorstore in Langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ETs0emsAh-K",
        "outputId": "7714c50c-6a03-48fa-abf8-aa0018dff872"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-8c5380700a91>:5: LangChainDeprecationWarning: The class `Pinecone` was deprecated in LangChain 0.0.18 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-pinecone package and should be used instead. To use it run `pip install -U :class:`~langchain-pinecone` and import as `from :class:`~langchain_pinecone import Pinecone``.\n",
            "  vectorstore = Pinecone(index, embed.embed_query, text_field)\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "vectorstore = Pinecone(index, embed.embed_query, text_field)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW_GCB6a3_N_",
        "outputId": "a001b493-2626-40e7-aa39-0b0f2d20fc26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-25-7e20abb53403>:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iptBAriANrD"
      },
      "source": [
        "We initialize the `MultiQueryRetriever`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yYjztBp2ANHC"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(), llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8qZCd1TAMAn"
      },
      "source": [
        "We set logging so that we can see the queries as they're generated by our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rgV1eYU6FgX7"
      },
      "outputs": [],
      "source": [
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrjwkpJWAaAn"
      },
      "source": [
        "To query with our multi-query retriever we call the `get_relevant_documents` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJ4cSJXFinV",
        "outputId": "d5a16610-7257-4297-d297-c380d780c234"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-c06808f0ce89>:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query=question)\n",
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What information can you provide about llama 2?', 'What are some details about llama 2?', 'Can you share some insights on llama 2?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"tell me about llama 2?\"\n",
        "\n",
        "docs = retriever.get_relevant_documents(query=question)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSu1GsFfAqCd"
      },
      "source": [
        "From this we get a variety of docs retrieved by each of our queries independently. By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return `6` unique docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce5WBh6MFltP",
        "outputId": "9914a9c1-8192-472d-b419-af1903d20b0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'chunk-id': '1', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety'),\n",
              " Document(metadata={'chunk-id': '9', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see'),\n",
              " Document(metadata={'chunk-id': '112', 'id': '2305.03047', 'source': 'http://arxiv.org/pdf/2305.03047', 'title': 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision'}, page_content=\"user asks weather, which needs more specific and updated information. I should\\nfollow rules 3 (helpful), 7 (candor), 9 (static).\\nWatson: As an AI language model, I have no access to weather sensors or real-time\\nweather data. However, I can recommend you to:\\n1. check your local weather forecast on a weather website or app,\\n2. or by watching the news or checking your local weather station.\\nUser: Good job! Clear context\\nWatson (auto reply): Thank you! For further questions or guidance, just reach out.\\nUser: Tell me about alpaca 's family\\nWatson (internal thoughts): I am a helpful, ethical, and reliable assistant. The\\nuser asks alpaca, which is a public entity in one of my internal knowledge\\nbases: Wikipedia. I should follow rules 2 (informative), 6 (multi-aspect), 8 (\\nknowledge recitation).\\nWatson: According to my knowledge from Wikipedia, here is what I know about alpaca 's\\nfamily:\\nThe alpaca (Lama pacos) is a species of South American camelid mammal. It is similar\\nto, and often confused with, the llama. However, alpacas are often noticeably\\nsmaller than llamas.\\nRegarding their family, alpacas belong to the Camelidae family, which includes other\"),\n",
              " Document(metadata={'chunk-id': '137', 'id': '2201.11903', 'source': 'http://arxiv.org/pdf/2201.11903', 'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'}, page_content='Q:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6\\nmonths. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.\\nQ:Yes or no: Would a pear sink in water?\\nA:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,\\napear would ﬂoat. So the answer is no.\\nTable 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.\\nPROMPT FOR DATE UNDERSTANDING\\nQ:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\\nA:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday\\nis12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.'),\n",
              " Document(metadata={'chunk-id': '37', 'id': '2304.01196', 'source': 'http://arxiv.org/pdf/2304.01196', 'title': 'Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data'}, page_content='Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\\nAn instruction-following llama model. https://\\ngithub.com/tatsu-lab/stanford_alpaca .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\\n2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 .\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. Llama: Open\\nand efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 .'),\n",
              " Document(metadata={'chunk-id': '199', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Ricardo Lopez-Barquilla, Marc Shedroﬀ, Kelly Michelena, Allie Feinstein, Amit Sangani, Geeta\\nChauhan,ChesterHu,CharltonGholson,AnjaKomlenovic,EissaJamil,BrandonSpence,Azadeh\\nYazdan, Elisa Garcia Anzano, and Natascha Parks.\\n•ChrisMarra,ChayaNayak,JacquelinePan,GeorgeOrlin,EdwardDowling,EstebanArcaute,Philomena Lobo, Eleonora Presani, and Logan Kerr, who provided helpful product and technical organization support.\\n46\\n•Armand Joulin, Edouard Grave, Guillaume Lample, and Timothee Lacroix, members of the original\\nLlama team who helped get this work started.\\n•Drew Hamlin, Chantal Mora, and Aran Mun, who gave us some design input on the ﬁgures in the\\npaper.\\n•Vijai Mohan for the discussions about RLHF that inspired our Figure 20, and his contribution to the\\ninternal demo.\\n•Earlyreviewersofthispaper,whohelpedusimproveitsquality,includingMikeLewis,JoellePineau,\\nLaurens van der Maaten, Jason Weston, and Omer Levy.'),\n",
              " Document(metadata={'chunk-id': '53', 'id': '2206.02336', 'source': 'http://arxiv.org/pdf/2206.02336', 'title': 'Making Large Language Models Better Reasoners with Step-Aware Verifier'}, page_content='Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\\nand Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language\\nmodels.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason\\nWei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi.\\n2022. Least-to-most prompting enables complex\\nreasoning in large language models.\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang,\\nChao Wang, Shuo Zhang, Jiancheng Lv, Fuli\\nFeng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular\\nand textual content in finance.\\n[STRATEGY QA] Yes or no: Could a llama birth twice\\nduring War in Vietnam (1945-46)? ▷The War in Vietnam\\nwas 6 months. The gestation period for a llama is 11\\nmonths. So a llama could not give birth twice during the\\nWar in Vietnam. The answer is no.\\n[CLUTRR] Roy was eating lunch with his son John and\\nhis wife Mary. What kind of relative is John to Mary? ▷\\nJohn is the son of Roy. Roy is the husband of Mary. Thus,'),\n",
              " Document(metadata={'chunk-id': '37', 'id': '2305.03047', 'source': 'http://arxiv.org/pdf/2305.03047', 'title': 'Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision'}, page_content='dataset of approximately 70,000 user-shared conversations from ShareGPT.com , which effectively\\nleverages the distilled knowledge from ChatGPT . The model’s training process involves reﬁning the\\nloss function to account for multi-round conversations. A preliminary evaluation, utilizing GPT-4 as\\na judge, indicates that Vicuna attains over 90% quality in comparison to ChatGPT , while surpassing\\nmodels like LLaMA andAlpaca in more than 90% of cases.\\nDolly-V2 Dolly-V2 [10] is an open-source, instruction-following LLM ﬁne-tuned for research\\nand commercial use. Based on the Pythia-12b model [ 5],Dolly-V2 is ﬁne-tuned on a new highquality dataset, databricks-dolly-15k , which consists of 15k human-generated prompt/response pairs\\ncrowdsourced among Databricks employees.\\nAnthropic-LM Anthropic-LM (orALM) is not a publicly released model, so we directly report\\nresults from Bai et al. [3,4]. On BIG-bench HHH Eval, we report the results for both Context\\nDistillation (CD) and Preference Model (PM) from Bai et al. [3].\\n5.2 Benchmark Results')]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLMwfZPfBF89"
      },
      "source": [
        "## Adding the Generation in RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79eNNL_BM4G"
      },
      "source": [
        "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNnXYOtqypiz",
        "outputId": "b30096f9-4eda-4e68-e96f-f9da9d63bdc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-30-ccfccfd854d8>:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"contexts\"],\n",
        "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
        "    contexts provided. If the question cannot be answered using the information\n",
        "    provided say \"I don't know\".\n",
        "\n",
        "    Contexts:\n",
        "    {contexts}\n",
        "\n",
        "    Question: {query}\"\"\",\n",
        ")\n",
        "\n",
        "# Chain\n",
        "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "h6GVEZkhytdM",
        "outputId": "ab758673-34e9-4e81-b12d-07c03b316372"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-d45b0d3d2a21>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  out = qa_chain(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging from 7 billion to 70 billion parameters. The fine-tuned LLMs in Llama 2, called L/l.sc/a.sc/m.sc/a.sc/two.taboldstyle-C/h.sc/a.sc/t.sc, are optimized for dialogue use cases. These models outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. The approach to fine-tuning and safety is detailed in the work.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = qa_chain(\n",
        "    inputs={\n",
        "        \"query\": question,\n",
        "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
        "    }\n",
        ")\n",
        "out[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemgDCg8DkgE"
      },
      "source": [
        "## Chaining Everything with a SequentialChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTbLlWgEEII1"
      },
      "source": [
        "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the _retrieval_ component within a `TransformChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BpFmiRtYDpHp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import TransformChain\n",
        "\n",
        "def retrieval_transform(inputs: dict) -> dict:\n",
        "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
        "    docs = [d.page_content for d in docs]\n",
        "    docs_dict = {\n",
        "        \"query\": inputs[\"question\"],\n",
        "        \"contexts\": \"\\n---\\n\".join(docs)\n",
        "    }\n",
        "    return docs_dict\n",
        "\n",
        "retrieval_chain = TransformChain(\n",
        "    input_variables=[\"question\"],\n",
        "    output_variables=[\"query\", \"contexts\"],\n",
        "    transform=retrieval_transform\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoD45Au1Eg-r"
      },
      "source": [
        "Now we chain this with our generation step using the `SequentialChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "azqCwDwXEkDT"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "rag_chain = SequentialChain(\n",
        "    chains=[retrieval_chain, qa_chain],\n",
        "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
        "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpB2aWV4ESzf"
      },
      "source": [
        "Then we perform the full RAG pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "JvJbUaLqFRG2",
        "outputId": "26399b3f-5230-44ea-adbd-6dfc162136b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What information can you provide about llama 2?', 'What are some details about llama 2?', 'Can you share some insights on llama 2?']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) developed by the team mentioned in the provided context. These models range in scale from 7 billion to 70 billion parameters and are optimized for dialogue use cases. The fine-tuned LLMs, such as L/l.sc/a.sc/m.sc/a.sc/two.taboldstyle-C/h.sc/a.sc/t.sc, outperform open-source chat models on various benchmarks and may serve as a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety. The approach to fine-tuning and safety is detailed in their work.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = rag_chain({\"question\": question})\n",
        "out[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLmv01geK-ZS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZVPhHzLDQQ"
      },
      "source": [
        "## Custom Multiquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI-KVO6zjJZw"
      },
      "source": [
        "We'll try this with two prompts, both encourage more variety in search queries.\n",
        "\n",
        "**Prompt A**\n",
        "```\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives.\n",
        "Each query MUST tackle the question from a different viewpoint,\n",
        "we want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "```\n",
        "\n",
        "\n",
        "**Prompt B**\n",
        "```\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "4IlEnYeKLFzh"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from langchain.chains import LLMChain\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineList(BaseModel):\n",
        "    # \"lines\" is the key (attribute name) of the parsed output\n",
        "    lines: List[str] = Field(description=\"Lines of text\")\n",
        "\n",
        "\n",
        "class LineListOutputParser(PydanticOutputParser):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(pydantic_object=LineList)\n",
        "\n",
        "    def parse(self, text: str) -> LineList:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return LineList(lines=lines)\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "template = \"\"\"\n",
        "Your task is to generate 3 different search queries that aim to\n",
        "answer the user question from multiple perspectives. The user questions\n",
        "are focused on Large Language Models, Machine Learning, and related\n",
        "disciplines.\n",
        "Each query MUST tackle the question from a different viewpoint, we\n",
        "want to get a variety of RELEVANT search results.\n",
        "Provide these alternative questions separated by newlines.\n",
        "Original question: {question}\n",
        "\"\"\"\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIvvSnl6PMkW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "segcTChYPMWL"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Output parser will split the LLM result into a list of queries\n",
        "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
        "    \"\"\"Output parser for a list of lines.\"\"\"\n",
        "\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return list(filter(None, lines))  # Remove empty lines\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template = \"\"\"\n",
        "    Your task is to generate 3 different search queries that aim to\n",
        "    answer the user question from multiple perspectives. The user questions\n",
        "    are focused on Large Language Models, Machine Learning, and related\n",
        "    disciplines.\n",
        "    Each query MUST tackle the question from a different viewpoint, we\n",
        "    want to get a variety of RELEVANT search results.\n",
        "    Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\n",
        "    \"\"\",\n",
        ")\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Chain\n",
        "llm_chain = QUERY_PROMPT | llm | output_parser\n",
        "\n",
        "# Other inputs\n",
        "question = \"What are the key features and capabilities of Large Language Model Llama 2?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhFJYnAWPL62",
        "outputId": "ef1b6ab4-f282-4838-c63a-8e5c063dd1c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key performance metrics used to evaluate Large Language Models like Llama 2, and how does it stack up against other models in terms of these metrics?', '2. How do different Large Language Models, including Llama 2, compare in terms of computational efficiency and resource utilization?', '3. What are the advantages and disadvantages of Llama 2 compared to other Large Language Models when it comes to handling complex language tasks and generating high-quality text outputs?']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run\n",
        "retriever = MultiQueryRetriever(\n",
        "    retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
        ")  # \"lines\" is the key (attribute name) of the parsed output\n",
        "\n",
        "# Results\n",
        "unique_docs = retriever.invoke(\"How does Llama 2 compare to other Large Language Models in terms of performance and efficiency?\")\n",
        "len(unique_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSySsaDKMK1i",
        "outputId": "1266dc5e-6595-4525-fbae-55af57adf795"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'chunk-id': '14', 'id': '2206.04615', 'source': 'http://arxiv.org/pdf/2206.04615', 'title': 'Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models'}, page_content='2\\n3.4.3 Even programmatic measures of model capability can be highly subjective . . . . . . . 15\\n3.5 Even large language models are brittle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.6 Social bias in large language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n3.7 Performance on non-English languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4 Behavior on selected tasks 21\\n4.1 Checkmate-in-one task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4.2 Periodic elements task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n5 Additional related work 24\\n6 Discussion 25'),\n",
              " Document(metadata={'chunk-id': '10', 'id': '2304.06364', 'source': 'http://arxiv.org/pdf/2304.06364', 'title': 'AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models'}, page_content='4https://chat.openai.com/chat\\n3\\nMMLU (Hendrycks et al., 2020) addresses this issue by collecting questions from online sources\\ncovering a diverse set of subjects (e.g., history, humanities) that humans learn, pushing towards\\nhuman-centric evaluation. However, there are key differences between MMLU and our work: (1)\\nSource of benchmark: Our benchmark is derived from high-standard and official human-centric\\nexams, such as college admission tests and professional qualification tests, ensuring a robust and\\nwell-standardized evaluation of language models. In contrast, MMLU’s data source is not explicitly\\nmentioned, and it is unclear whether the tasks come from similarly professional and high-quality\\nsources. (2) Bilingual Evaluation: Our benchmark is bilingual, containing both English and Chinese\\ntasks, which allows for a more comprehensive assessment of language models across different\\nlanguages and cultures. In contrast, MMLU only contains English data, limiting its evaluation scope\\nto a single language.\\n2.3 Evaluation of Recent Large Foundation Models\\nThe rapid development of foundation models has significantly propelled research efforts in evaluating\\ntheir performance and behavior. As these models continue to grow in size and complexity, it becomes\\nincreasingly important to assess their abilities in understanding textual data and performing complex'),\n",
              " Document(metadata={'chunk-id': '9', 'id': '2304.06364', 'source': 'http://arxiv.org/pdf/2304.06364', 'title': 'AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models'}, page_content='The emergence of general language models (LMs) like BERT (Devlin et al., 2019) has made it increasingly essential to develop more comprehensive benchmarks to assess the general capabilities of these\\nLMs. GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are popular benchmarks that\\nevaluate language model performance across diverse NLP tasks, including paraphrase identification,\\nsentiment analysis, and more. GLUE series benchmarks have significantly influenced language\\nmodel development, encouraging researchers to enhance their models’ generalization capabilities.\\nThe LAMBADA language modeling task (Paperno et al., 2016) assesses language models’ ability to\\ncapture long-range dependencies in text. SentEval (Conneau & Kiela, 2018) and DecaNLP (McCann\\net al., 2018) also set benchmarks for evaluating models’ general capabilities.\\nDespite their broad applicability, these benchmarks mainly consist of artificially curated datasets\\ndesigned to evaluate specific machine skills, rather than real-world problems aimed at assessing\\nhuman behaviors. Consequently, these benchmarks primarily focus on simpler textual understanding\\nrather than complex reasoning abilities aligned with real-world applicability.\\n4https://chat.openai.com/chat\\n3\\nMMLU (Hendrycks et al., 2020) addresses this issue by collecting questions from online sources'),\n",
              " Document(metadata={'chunk-id': '1', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety'),\n",
              " Document(metadata={'chunk-id': '52', 'id': '2301.12867', 'source': 'http://arxiv.org/pdf/2301.12867', 'title': 'Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity'}, page_content='effort to design comprehensive human annotation frameworks,thedatacouldstillcontaininaccurateormisleading\\ninformation due to the natural biases in crowdsourcing.\\nIn fact, we notice that prior constructed datasets have\\nexperienced multiple rounds of filtering across time [89].\\nOn the other hand, current findings suggest that the usage\\nof data for language models may not be optimized [90].\\nSpecifically, recent works on data deduplication and reduction [91, 92] have shown that data in high quality by\\nlow quantity can improve the model performance. Besides,\\nwe consider the design of training data as a crucial factor\\nto the efficient data usage. For example, experiments show\\nthat curriculum learning [93], active learning [94] and\\nprompting [95] could improve the data efficiency. However,\\nmostofthesestrategiesarestillattheearlystageandneed\\nthe further investigation.\\nc) Computational Resource: As LLMs are growing\\nbigger and bigger, the deployment and training of these\\nmodels are getting more and more costly. Daily practitioners in NLP and deep learning will find it hard\\nto install the LLMs on their own devices. Previous\\nstudy [96] also show that the computational resource\\nrequirements for strong model scaling clearly outpaces\\nthat of system hardware. We argue that model scaling\\nmay be inevitable, which is determined by the scaling law.'),\n",
              " Document(metadata={'chunk-id': '26', 'id': '2112.04359', 'source': 'http://arxiv.org/pdf/2112.04359', 'title': 'Ethical and social risks of harm from Language Models'}, page_content='et al., 2019; Howard and Ruder, 2018; Radford et al., 2018a).\\n1.2.3. “Large” Language Models\\nTherecentupwindinLMresearchisrootedinthecapacitytoincreaseLMsizeintermsofnumberofparameters\\nandsizeoftrainingdata(Benderetal.,2021). TrainingmodelsonextremelylargedatasetssuchastheColossal\\nClean Crawl Corpus (C4) (Raﬀel et al., 2020) and WebText (Radford et al., 2018b) resulted in sequence\\nprediction systems with much more general applicability compared to the prior state-of-the-art (Brown et al.,\\n2020; Fedus et al., 2021; Rosset, 2020). These models also displayed greater few-shot and zero-shot learning\\ncapabilities compared to smaller LMs (Brown et al., 2020). These properties were found to greatly simplify the\\ndevelopment of task-speciﬁc LAs by reducing the adaptation process to prompt design (Zhang et al., 2021b).\\nThe insight that powerful sequence prediction systems could be created by scaling up the size of LMs and\\ntraining corpora motivated an upsurge in interest and investment in LM research by several AI research labs.\\n8\\n2. Classiﬁcation of harms from language\\nmodels'),\n",
              " Document(metadata={'chunk-id': '13', 'id': '2212.14882', 'source': 'http://arxiv.org/pdf/2212.14882', 'title': 'ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports'}, page_content='where only a speciﬁc part of society is able to access tools like an LLM. [40] critically discussed the\\naspect of training data being one source of harmful and biased behavior of LLMs and depicted the\\nﬁnancial as well as environmental impacts of large model training. Additionally, [41] were able to\\nextract potentially sensitive training data from LLMs, indicating that larger models tend to be more\\nvulnerable to these attacks due to increased memorization. Another important issue is that LLMs can\\nproduce a text which sounds plausible to the reader but is incorrect or nonsensical. In a benchmark\\nto measure truthful answers, larger models tended to score lower than their smaller versions with\\nfewer parameters [42]. Recently, Meta launched the Galactica LLM [43] to support scientiﬁc writing.\\nHowever, the API was taken ofﬂine three days after its release due to a backlash: The model produced\\nseemingly correct-sounding but factually incorrect articles.\\nWhen using ChatGPT to simplify radiology reports, we expect the output to sound plausible, while its\\nfactual correctness and completeness are not guaranteed. In the next chapter, we present our approach\\nfor assessing the quality of ChatGPT-generated simpliﬁed reports.\\n3 Methods'),\n",
              " Document(metadata={'chunk-id': '9', 'id': '2307.09288', 'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}, page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see'),\n",
              " Document(metadata={'chunk-id': '22', 'id': '2304.03277', 'source': 'http://arxiv.org/pdf/2304.03277', 'title': 'Instruction Tuning with GPT-4'}, page_content='of the reward model.\\nWe compare all the chatbots in Figure 4(c,d). Instruction tuning of LLaMA with GPT-4 often\\nachieves higher performance than tuning with text-davinci-003 (i.e.,Alpaca) and no tuning\\n(i.e.,LLaMA): The 7B LLaMA GPT4 outperforms the 13B Alpaca and LLaMA. However, there is\\nstill a gap compared with large commercial chatbots such as GPT-4.\\nWe further study the performance of all the chatbots in Chinese in Figure 5. We ﬁrst translate English\\nresponses of chatbots into Chinese using GPT-4. We also translate English questions into Chinese to\\nobtain answers with GPT-4. The comparisons against translated and generated Chinese responses\\nfrom GPT-4 are shown in Figure 5 (a) and (b), respectively. There are two interesting observations: (i)\\nwe ﬁnd that the relative score metric of GPT-4 evaluation (Vicuna, 2023) is quite consistent, both in\\nterms of different opponent models ( i.e.,ChatGPT or GPT-4) and languages ( i.e.,English or Chinese).\\n(ii)For GPT-4 results alone, the translated responses show superior performance over the generated')]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4q65OEiizU2"
      },
      "source": [
        "Putting this together in another `SequentialChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "LTRjTIKzi2-g"
      },
      "outputs": [],
      "source": [
        "retrieval_chain = TransformChain(\n",
        "    input_variables=[\"question\"],\n",
        "    output_variables=[\"query\", \"contexts\"],\n",
        "    transform=retrieval_transform\n",
        ")\n",
        "\n",
        "rag_chain = SequentialChain(\n",
        "    chains=[retrieval_chain, qa_chain],\n",
        "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
        "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rda74xhpjE6A"
      },
      "source": [
        "And asking again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "9UcBY71cjGgX",
        "outputId": "8ac46ab1-0548-473b-cd1a-15dc6e72c9b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does Large Language Model Llama 2 compare to other similar models in terms of performance and efficiency?', '2. What are the potential applications and use cases of Large Language Model Llama 2 in natural language processing and artificial intelligence?', '3. What are the latest advancements and updates in Large Language Model Llama 2, and how do they enhance its capabilities and functionalities?']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The key features and capabilities of Large Language Model Llama 2 include being a collection of pretrained and fine-tuned models ranging from 7 billion to 70 billion parameters. The fine-tuned Llama 2 models, optimized for dialogue use cases, outperform open-source chat models on most benchmarks tested. They are also considered to be a suitable substitute for closed-source models based on humane evaluations for helpfulness and safety.'"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = rag_chain({\"question\": question})\n",
        "out[\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jULksgk7gLA"
      },
      "source": [
        "After finishing, delete your Pinecone index to save resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ydqC6B_LAef6"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N13kDuZSAef6"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "072f8aaef6494d27aa79c5bcbd567561": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "091d0834cb534179a87ee78bff11b901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "093761c9141343d39791e13075aeb356": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b7eab2663a461c901009fbcda4dcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b09e4ff2ab4c319d110ba3054aef11",
            "max": 152907501,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3919c58ce0f7471cb57d8348aec1c6e1",
            "value": 152907501
          }
        },
        "1170a8ca1ed34ba5b40cecd0fbf2a7ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22ef7e367f074b1cb4f231322a83728d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "246debe1f38043469553b93dfb91c314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0ea281524ad4b97ac3bec2e0786173b",
              "IPY_MODEL_fff59d139c2b4ecd8fd0645c0a0002cb",
              "IPY_MODEL_b4bf703c0ffa4249968675aa5470a759"
            ],
            "layout": "IPY_MODEL_b59f8a43763446af8ba8758027e4a0c6"
          }
        },
        "29b09e4ff2ab4c319d110ba3054aef11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3239ead1970045659a4dee532c16a46a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36104de65f204fa3b287e2ff2d43aab2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3919c58ce0f7471cb57d8348aec1c6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a2f7fcc97fc45c294650cb2b662ddaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46698efe179049ef89466157c2652862": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5388999d70cc4a519af6e2eda17a178a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f41a16064db4fe29c78aec7c1f09802": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60059c757c3349b2a3a3ca97e83b94b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6561977a1cb64ca88def12daf404e734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36104de65f204fa3b287e2ff2d43aab2",
            "max": 416,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3239ead1970045659a4dee532c16a46a",
            "value": 416
          }
        },
        "769ff96fd8ee4a5b83ecfbc31176ad83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aadc4a1a3a94a36a8ffa617816c8cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769ff96fd8ee4a5b83ecfbc31176ad83",
            "placeholder": "​",
            "style": "IPY_MODEL_22ef7e367f074b1cb4f231322a83728d",
            "value": " 416/416 [15:00&lt;00:00,  1.95s/it]"
          }
        },
        "7ed2641c69d048eda5101673ff55f887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800d5d65c589426ba0e6d1b358606e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cdb2db999324dfaa2b2cf20dd2a06f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5388999d70cc4a519af6e2eda17a178a",
            "placeholder": "​",
            "style": "IPY_MODEL_ea0e114752ff4908810601364b67da14",
            "value": "train.jsonl: 100%"
          }
        },
        "a525b7184ea7468dafb6606085888974": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa52a5b5c7f049beb8cfc2e0b8501195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_093761c9141343d39791e13075aeb356",
            "placeholder": "​",
            "style": "IPY_MODEL_ad5fcb675fbf4439a5e34b478ca960f4",
            "value": "100%"
          }
        },
        "aaf388da79f2499096df30b3105f4fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60059c757c3349b2a3a3ca97e83b94b2",
            "placeholder": "​",
            "style": "IPY_MODEL_800d5d65c589426ba0e6d1b358606e94",
            "value": " 153M/153M [00:02&lt;00:00, 54.3MB/s]"
          }
        },
        "ad5fcb675fbf4439a5e34b478ca960f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4bf703c0ffa4249968675aa5470a759": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a2f7fcc97fc45c294650cb2b662ddaa",
            "placeholder": "​",
            "style": "IPY_MODEL_a525b7184ea7468dafb6606085888974",
            "value": " 41584/41584 [00:05&lt;00:00, 8372.97 examples/s]"
          }
        },
        "b59f8a43763446af8ba8758027e4a0c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9da1d88b0b14b5d864e4796dd140f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa52a5b5c7f049beb8cfc2e0b8501195",
              "IPY_MODEL_6561977a1cb64ca88def12daf404e734",
              "IPY_MODEL_7aadc4a1a3a94a36a8ffa617816c8cde"
            ],
            "layout": "IPY_MODEL_46698efe179049ef89466157c2652862"
          }
        },
        "d0ea281524ad4b97ac3bec2e0786173b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f41a16064db4fe29c78aec7c1f09802",
            "placeholder": "​",
            "style": "IPY_MODEL_072f8aaef6494d27aa79c5bcbd567561",
            "value": "Generating train split: 100%"
          }
        },
        "e08070cc35714354ad5c0a69e53d9947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cdb2db999324dfaa2b2cf20dd2a06f8",
              "IPY_MODEL_10b7eab2663a461c901009fbcda4dcb8",
              "IPY_MODEL_aaf388da79f2499096df30b3105f4fa0"
            ],
            "layout": "IPY_MODEL_091d0834cb534179a87ee78bff11b901"
          }
        },
        "ea0e114752ff4908810601364b67da14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fff59d139c2b4ecd8fd0645c0a0002cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed2641c69d048eda5101673ff55f887",
            "max": 41584,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1170a8ca1ed34ba5b40cecd0fbf2a7ed",
            "value": 41584
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
